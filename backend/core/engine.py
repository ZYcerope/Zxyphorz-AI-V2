   from __future__ import annotations

import re
import uuid
from dataclasses import dataclass
from typing import Any, Dict, Iterator, List, Optional, Tuple

from .i18n import detect_language, lang_name, normalize_lang, tr
from .memory_store import MemoryStore
from .persona import Persona
from .rag import KnowledgeBase
from .tools.registry import ToolRegistry
from .tools.summarize import _summarize
from .utils import Timer, normalize_ws

try:
    from ..llm.local_slm import LocalSLM
except Exception:  # pragma: no cover
    LocalSLM = None  # type: ignore


@dataclass
class ChatResponse:
    session_id: str
    reply: str
    meta: Dict[str, Any]


class ChatEngine:
    """Zxyphorz AI engine.

    Modes
    - basic: deterministic tools + KB retrieval + memory
    - advanced: same tools/memory/KB, but final answer is generated by a local SLM
      loaded from a GGUF file via llama-cpp-python.
    """

    def __init__(
        self,
        persona: Persona,
        store: MemoryStore,
        kb: KnowledgeBase,
        tools: ToolRegistry,
        seed_facts: Optional[Dict[str, Any]] = None,
        slm: Optional[object] = None,
    ):
        self.persona = persona
        self.store = store
        self.kb = kb
        self.tools = tools
        self.seed_facts = seed_facts or {}
        self.slm = slm  # LocalSLM or None

    @staticmethod
    def new_session_id() -> str:
        return uuid.uuid4().hex[:16]

    def handle(
        self,
        user_text: str,
        session_id: Optional[str],
        language: Optional[str] = None,
        mode: str = "basic",
    ) -> ChatResponse:
        t_total = Timer.start_now()
        session_id = session_id or self.new_session_id()
        user_text = (user_text or "").strip()
        if not user_text:
            return ChatResponse(session_id, "Say something and I’ll respond.", {"ms": t_total.ms(), "mode": "basic"})

        self._ensure_seeded(session_id)
        effective_lang = self._resolve_language(session_id, user_text, language)

        self.store.add_message(session_id, "user", user_text)
        self._extract_facts(session_id, user_text)

        if user_text.startswith("/"):
            reply = self._handle_command(session_id, user_text, effective_lang)
            self.store.add_message(session_id, "assistant", reply)
            return ChatResponse(session_id, reply, {"ms": t_total.ms(), "mode": "command", "lang": effective_lang})

        tool_res = self.tools.run_first(user_text, session_id)
        if tool_res is not None:
            reply = tool_res.text
            self.store.add_message(session_id, "assistant", reply)
            self._maybe_refresh_summary_fact(session_id)
            meta = {"ms": t_total.ms(), "mode": "tool", "lang": effective_lang, **tool_res.meta}
            return ChatResponse(session_id, reply, meta)

        mode_norm = (mode or "basic").strip().lower()
        if mode_norm == "advanced":
            reply, meta = self._advanced_chat_reply(session_id, user_text, effective_lang)
            self.store.add_message(session_id, "assistant", reply)
            self._maybe_refresh_summary_fact(session_id)
            meta["ms"] = t_total.ms()
            meta["mode"] = "advanced"
            meta["lang"] = effective_lang
            return ChatResponse(session_id, reply, meta)

        reply, meta = self._chat_reply(session_id, user_text, effective_lang)
        self.store.add_message(session_id, "assistant", reply)
        self._maybe_refresh_summary_fact(session_id)
        meta["ms"] = t_total.ms()
        meta["mode"] = "basic"
        meta["lang"] = effective_lang
        return ChatResponse(session_id, reply, meta)

    def handle_stream(
        self,
        user_text: str,
        session_id: Optional[str],
        language: Optional[str] = None,
        mode: str = "basic",
    ) -> Tuple[str, Dict[str, Any], Iterator[str]]:
        t_total = Timer.start_now()
        session_id = session_id or self.new_session_id()
        user_text = (user_text or "").strip()
        if not user_text:
            def _empty():
                yield "Say something and I’ll respond."
            return session_id, {"mode": "basic", "lang": "en", "ms": 0}, _empty()

        self._ensure_seeded(session_id)
        effective_lang = self._resolve_language(session_id, user_text, language)
        self.store.add_message(session_id, "user", user_text)
        self._extract_facts(session_id, user_text)

        if user_text.startswith("/"):
            reply = self._handle_command(session_id, user_text, effective_lang)
            return self._stream_from_text(session_id, reply, t_total, effective_lang, mode="command")

        tool_res = self.tools.run_first(user_text, session_id)
        if tool_res is not None:
            return self._stream_from_text(session_id, tool_res.text, t_total, effective_lang, mode="tool", extra_meta=tool_res.meta)

        mode_norm = (mode or "basic").strip().lower()
        if mode_norm == "advanced":
            return self._stream_advanced(session_id, user_text, t_total, effective_lang)

        reply, meta = self._chat_reply(session_id, user_text, effective_lang)
        return self._stream_from_text(session_id, reply, t_total, effective_lang, mode="basic", extra_meta=meta)

    # ---------------------- Seed memory ----------------------
    def _ensure_seeded(self, session_id: str) -> None:
        facts = self.store.list_facts(session_id)
        if (facts.get("__seeded__") or {}).get("value") == "1":
            return

        for k, v in self.seed_facts.items():
            val = str(v)
            self.store.upsert_fact(session_id, k, val, confidence=1.0)

        self.store.upsert_fact(session_id, "__seeded__", "1", confidence=1.0)

    # ---------------------- Language ----------------------
    def _resolve_language(self, session_id: str, user_text: str, language: Optional[str]) -> str:
        req_lang = normalize_lang(language)
        if req_lang:
            self.store.upsert_fact(session_id, "preferred_language", req_lang, confidence=1.0)
            return req_lang

        facts = self.store.list_facts(session_id)
        pref = normalize_lang((facts.get("preferred_language") or {}).get("value"))
        if pref:
            return pref

        guess = detect_language(user_text)
        if guess.confidence >= 0.75:
            return guess.code
        return "en"

    # ---------------------- Commands ----------------------
    def _handle_command(self, session_id: str, cmd: str, lang: str) -> str:
        raw = cmd.strip()
        c = raw.strip().lower()

        if c in {"/help", "/?"}:
            return self.tools.help_text()

        if c == "/reset":
            self.store.reset_session(session_id)
            return tr(lang, "session_cleared")

        if c == "/export":
            return "Use the UI button 'Export' or open `/api/export?session_id=...` to download JSON."

        if c == "/memory":
            facts = self.store.list_facts(session_id)
            user_facts = {k: v for k, v in facts.items() if not k.startswith("__")}
            if not user_facts:
                return tr(lang, "no_facts")
            lines = [tr(lang, "saved_facts")]
            for k, v in user_facts.items():
                lines.append(f"- **{k}**: {v['value']} _(confidence {v['confidence']:.2f})_")
            return "\n".join(lines)

        if c.startswith("/lang"):
            parts = raw.split()
            if len(parts) == 1:
                cur = normalize_lang((self.store.list_facts(session_id).get("preferred_language") or {}).get("value")) or "en"
                return tr(lang, "language_show", lang_name=lang_name(cur), lang_code=cur) + "\n" + tr(lang, "language_help")
            target = normalize_lang(parts[1])
            if not target:
                return tr(lang, "language_help")
            self.store.upsert_fact(session_id, "preferred_language", target, confidence=1.0)
            return tr(lang, "language_set", lang_name=lang_name(target), lang_code=target)

        if c in {"/packs", "/knowledge"}:
            return (
                "To manage offline knowledge packs, use:\n"
                "- `packs list`\n"
                "- `packs status`\n"
                "- `packs howto`\n"
                "Or run: `python scripts/packs.py list`"
            )

        return tr(lang, "unknown_command")

    # ---------------------- Facts ----------------------
    def _extract_facts(self, session_id: str, user_text: str) -> None:
        text = user_text.strip()
        lower = text.lower()

        m = re.search(r"\bmy name is\s+([A-Za-z0-9_\- ]{2,40})\b", text, flags=re.I)
        if m:
            self.store.upsert_fact(session_id, "user_name", m.group(1).strip(), confidence=0.95)

        m = re.search(r"\bcall me\s+([A-Za-z0-9_\- ]{2,40})\b", text, flags=re.I)
        if m:
            self.store.upsert_fact(session_id, "preferred_name", m.group(1).strip(), confidence=0.9)

        m = re.search(r"\bnama\s+saya\s+([A-Za-z0-9_\- ]{2,40})\b", text, flags=re.I)
        if m:
            self.store.upsert_fact(session_id, "user_name", m.group(1).strip(), confidence=0.95)

        m = re.search(r"\baku\s+bernama\s+([A-Za-z0-9_\- ]{2,40})\b", text, flags=re.I)
        if m:
            self.store.upsert_fact(session_id, "user_name", m.group(1).strip(), confidence=0.95)

        m = re.search(r"\btimezone\s*(?:is|:)\s*([A-Za-z_\-/]+)\b", text, flags=re.I)
        if m:
            self.store.upsert_fact(session_id, "timezone", m.group(1).strip(), confidence=0.8)

        if "zxyphorz ai" in lower:
            self.store.upsert_fact(session_id, "assistant_name", "Zxyphorz AI", confidence=1.0)

        m = re.search(r"\b(speak|respond|reply)\s+in\s+(english|indonesian|bahasa|spanish|french|portuguese|chinese|mandarin|japanese)\b", lower)
        if m:
            target = normalize_lang(m.group(2))
            if target:
                self.store.upsert_fact(session_id, "preferred_language", target, confidence=0.9)

    def _maybe_refresh_summary_fact(self, session_id: str) -> None:
        recent = self.store.recent_messages(session_id, limit=30)
        if len(recent) < 12:
            return
        assistant_turns = sum(1 for m in recent if m.role == "assistant")
        if assistant_turns % 10 != 0:
            return

        transcript = []
        for m in recent[-18:]:
            transcript.append(f"{m.role.upper()}: {m.content}")
        summary = _summarize(" ".join(transcript), max_sentences=3)
        if summary:
            self.store.upsert_fact(session_id, "conversation_summary", summary, confidence=0.65)

    # ---------------------- Basic chat mode ----------------------
    def _chat_reply(self, session_id: str, user_text: str, lang: str) -> Tuple[str, Dict[str, Any]]:
        facts = self.store.list_facts(session_id)
        preferred_name = (facts.get("preferred_name") or {}).get("value") or (facts.get("user_name") or {}).get("value")
        user_name_line = tr(lang, "greeting_named", name=preferred_name) if preferred_name else tr(lang, "greeting_generic")

        hits = self.kb.search(user_text, k=4, lang_hint=lang)
        sources = []
        context_bits = []
        for c, score in hits:
            sources.append({"title": c.title, "file": c.source_file, "lang": c.lang, "score": round(score, 4)})
            context_bits.append(f"- {c.text} (Source: {c.title})")

        summary = (facts.get("conversation_summary") or {}).get("value")

        lower = user_text.lower()
        is_question = user_text.strip().endswith("?") or lower.startswith(("what", "why", "how", "can", "could", "do ", "does ", "apa", "bagaimana", "kenapa"))
        wants_project = any(k in lower for k in ["project", "repo", "github", "portfolio", "build", "buatkan"])
        wants_help = any(k in lower for k in ["help", "guide", "steps", "how to", "tolong", "cara"])

        lines: List[str] = []
        lines.append(user_name_line)
        lines.append("")

        if wants_project:
            lines.append("If you're building a portfolio repo, here’s a strong path:")
            lines.append("- Pick one core feature and make it polished (docs, tests, clean UI).")
            lines.append("- Add 2–4 supporting features that show engineering depth.")
            lines.append("- Keep everything runnable with one command.")
        elif is_question or wants_help:
            lines.append("Here’s what I can tell you:")
        else:
            lines.append("Got it. Here’s my best response:")

        if context_bits:
            lines.append("")
            lines.append(f"### {tr(lang, 'context_title')}")
            lines.extend(context_bits[:4])

            lines.append("")
            lines.append(f"### {tr(lang, 'answer_title')}")
            lines.append(self._compose_answer(user_text, context_bits, summary=summary))
        else:
            lines.append("")
            lines.append(self._fallback_answer(user_text))

        lines.append("")
        lines.append(f"### {tr(lang, 'next_actions_title')}")
        lines.append(f"- {tr(lang, 'next_action_1')}")
        lines.append(f"- {tr(lang, 'next_action_2')}")

        return "\n".join(lines), {"sources": sources, "has_kb": bool(context_bits)}

    def _compose_answer(self, user_text: str, context_bits: List[str], summary: Optional[str]) -> str:
        q = normalize_ws(user_text)
        intro = (
            f"Based on what you asked — **{q}** — the key idea is to use the most relevant information "
            f"and apply it in a clear, step-by-step way."
        )
        if summary:
            intro += f" (Quick context from our earlier chat: {summary})"

        tips = [
            "Start small, then iterate—clean structure beats random complexity.",
            "Keep inputs/outputs explicit so behavior stays predictable.",
            "Use retrieval (your knowledge packs) when you need real-world coverage offline.",
            "Add lightweight checks and clear error messages for reliability.",
        ]
        return intro + "\n\n" + "\n".join(f"- {t}" for t in tips)

    def _fallback_answer(self, user_text: str) -> str:
        t = user_text.strip()
        lower = t.lower()

        if any(k in lower for k in ["hello", "hi", "halo"]):
            return "Hello! Tell me what you want to build or learn, and I’ll guide you."

        if "?" in t:
            return (
                "I might need a tiny bit more detail to answer precisely.\n\n"
                "Try adding:\n"
                "- what you’re trying to achieve\n"
                "- any constraints (offline, speed, UI, etc.)\n"
                "- what you’ve tried so far"
            )

        return (
            "I understand. If you tell me your exact goal, I can respond with a concrete plan.\n"
            "Example: “I want a local AI assistant with notes + todo + modern web UI.”"
        )

    # ---------------------- Advanced mode (local SLM) ----------------------
    def _advanced_chat_reply(self, session_id: str, user_text: str, lang: str) -> Tuple[str, Dict[str, Any]]:
        slm = self.slm
        try:
            st = slm.status() if slm is not None else None
        except Exception:
            st = None

        if slm is None or st is None or not getattr(st, "available", False):
            base_reply, base_meta = self._chat_reply(session_id, user_text, lang)
            note = "\n\n[Advanced mode is unavailable on this machine. Falling back to Basic.]"
            return base_reply + note, {**base_meta, "advanced_available": False}

        facts = self.store.list_facts(session_id)
        summary = (facts.get("conversation_summary") or {}).get("value") or ""
        preferred_name = (facts.get("preferred_name") or {}).get("value") or (facts.get("user_name") or {}).get("value") or ""

        hits = self.kb.search(user_text, k=5, lang_hint=lang)
        ctx_lines = []
        src_lines = []
        for c, score in hits[:4]:
            ctx_lines.append(f"- {c.text}")
            src_lines.append(f"- {c.title} (score {score:.3f})")

        system = self.persona.system_prompt()
        system += "\nADDITIONAL RULES:\n"
        system += f"- Output language: {lang}. If lang is en, write native English.\n"
        system += "- If the user selected a language other than English, respond in that language.\n"
        system += "- If you use provided context, keep it factual and do not invent sources.\n"
        system += "- Keep latency low: prefer short paragraphs and bullet points.\n"

        user_block = []
        if preferred_name:
            user_block.append(f"User name (if needed): {preferred_name}")
        if summary:
            user_block.append(f"Conversation summary: {summary}")

        user_block.append(f"User query: {user_text}")
        if ctx_lines:
            user_block.append("\nRelevant context (offline KB):\n" + "\n".join(ctx_lines))
            user_block.append("\nSources (for reference):\n" + "\n".join(src_lines))

        messages = [
            {"role": "system", "content": system},
            {"role": "user", "content": "\n".join(user_block)},
        ]

        reply, meta = slm.generate_chat(messages, stop=["\n\nUser query:"])
        if not reply.strip():
            base_reply, base_meta = self._chat_reply(session_id, user_text, lang)
            note = "\n\n[Advanced mode returned empty output. Falling back to Basic.]"
            return base_reply + note, {**base_meta, "advanced_available": True}

        if src_lines:
            reply = reply.strip() + "\n\n### Sources\n" + "\n".join(src_lines[:4])

        meta_out = {"advanced_available": True}
        if isinstance(meta, dict):
            meta_out.update(meta)
        return reply, meta_out

    def _stream_advanced(self, session_id: str, user_text: str, t_total: Timer, lang: str) -> Tuple[str, Dict[str, Any], Iterator[str]]:
        slm = self.slm
        try:
            st = slm.status() if slm is not None else None
        except Exception:
            st = None

        if slm is None or st is None or not getattr(st, "available", False):
            reply, meta = self._chat_reply(session_id, user_text, lang)
            reply += "\n\n[Advanced mode unavailable. Falling back to Basic.]"
            return self._stream_from_text(session_id, reply, t_total, lang, mode="advanced", extra_meta={**meta, "advanced_available": False})

        facts = self.store.list_facts(session_id)
        summary = (facts.get("conversation_summary") or {}).get("value") or ""
        preferred_name = (facts.get("preferred_name") or {}).get("value") or (facts.get("user_name") or {}).get("value") or ""

        hits = self.kb.search(user_text, k=5, lang_hint=lang)
        ctx_lines = []
        src_lines = []
        for c, score in hits[:4]:
            ctx_lines.append(f"- {c.text}")
            src_lines.append(f"- {c.title} (score {score:.3f})")

        system = self.persona.system_prompt()
        system += "\nADDITIONAL RULES:\n"
        system += f"- Output language: {lang}. If lang is en, write native English.\n"
        system += "- If the user selected a language other than English, respond in that language.\n"
        system += "- Keep latency low: prefer short paragraphs and bullet points.\n"

        user_block = []
        if preferred_name:
            user_block.append(f"User name (if needed): {preferred_name}")
        if summary:
            user_block.append(f"Conversation summary: {summary}")
        user_block.append(f"User query: {user_text}")
        if ctx_lines:
            user_block.append("\nRelevant context (offline KB):\n" + "\n".join(ctx_lines))
            user_block.append("\nSources (for reference):\n" + "\n".join(src_lines))

        messages = [
            {"role": "system", "content": system},
            {"role": "user", "content": "\n".join(user_block)},
        ]

        def gen() -> Iterator[str]:
            parts: Li